{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, multilabel_confusion_matrix, roc_curve, auc\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_examples(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['text'])\n",
    "    tokenized_inputs['labels'] = examples['labels']\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom batch preprocessor\n",
    "def collate_fn(batch, tokenizer):\n",
    "    dict_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "    d = {k: [dic[k] for dic in batch] for k in dict_keys}\n",
    "    d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['attention_mask'], batch_first=True, padding_value=0\n",
    "    )\n",
    "    d['labels'] = torch.stack(d['labels'])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.sigmoid(torch.tensor(logits)).numpy() > 0.5\n",
    "    labels = labels.numpy()\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    f1_micro = f1_score(labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "    # Plot Confusion Matrix for each label\n",
    "    conf_matrices = multilabel_confusion_matrix(labels, predictions)\n",
    "    fig, ax = plt.subplots(1, len(conf_matrices), figsize=(15, 5))\n",
    "    if len(conf_matrices) > 1:\n",
    "        for idx, cm in enumerate(conf_matrices):\n",
    "            plot_confusion_matrix(cm, idx, ax[idx])\n",
    "    else:\n",
    "        plot_confusion_matrix(conf_matrices[0], 0, ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curves\n",
    "    plot_multilabel_roc(labels, torch.sigmoid(torch.tensor(logits)).numpy(), num_classes=labels.shape[1])\n",
    "    \n",
    "    return {'f1_micro': f1_micro, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom trainer class to be able to pass label weights and calculate mutilabel loss\n",
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, label_weights, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.label_weights = label_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # compute custom loss\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.to(torch.float32), pos_weight=self.label_weights)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('train.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile, delimiter=','))\n",
    "    header_row = data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape\n",
    "idx, text, labels = list(zip(*[(int(row[0]), f'Title: {row[1].strip()}\\n\\nAbstract: {row[2].strip()}', row[3:]) for row in data]))\n",
    "labels = np.array(labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label weights\n",
    "label_weights = 1 - labels.sum(axis=0) / labels.sum()\n",
    "\n",
    "# stratified train test split for multilabel ds\n",
    "row_ids = np.arange(len(labels))\n",
    "train_idx, y_train, val_idx, y_val = iterative_train_test_split(row_ids[:,np.newaxis], labels, test_size = 0.1)\n",
    "x_train = [text[i] for i in train_idx.flatten()]\n",
    "x_val = [text[i] for i in val_idx.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hf dataset\n",
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': x_train, 'labels': y_train}),\n",
    "    'val': Dataset.from_dict({'text': x_val, 'labels': y_val})\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset with tokenizer\n",
    "def tokenize_examples(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['text'])\n",
    "    tokenized_inputs['labels'] = examples['labels']\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\n",
    "tokenized_ds = tokenized_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant=True,  # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # optimized fp format for ML\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # the dimension of the low-rank matrices\n",
    "    lora_alpha=8,  # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout=0.05,  # dropout probability of the LoRA layers\n",
    "    bias='none',  # whether to train bias weights, so 'none' for attention layers\n",
    "    task_type='SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=labels.shape[1]\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'multilabel_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8, # tested with 16gb gpu ram\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 10,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = functools.partial(collate_fn, tokenizer=tokenizer),\n",
    "    compute_metrics = compute_metrics,\n",
    "    label_weights = torch.tensor(label_weights, device=model.device)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "peft_model_id = 'multilabel_mistral'\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "def plot_confusion_matrix(conf_matrix, class_idx, ax, class_names=[\"Absent\", \"Present\"]):\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(f'Class {class_idx}')\n",
    "    ax.xaxis.set_ticklabels(class_names)\n",
    "    ax.yaxis.set_ticklabels(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_multilabel_roc(labels, predictions, num_classes):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels[:, i], predictions[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    colors = cycle(['blue', 'red', 'green', 'yellow', 'cyan', 'magenta', 'black'])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, color in zip(range(num_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic for Multi-label')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
